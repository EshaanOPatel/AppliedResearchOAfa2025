# Knowledge Distillation: Teaching Small Models to Think Big

**Project Overview**

This project applied knowledge distillation using gradient descent optimization.
The student model (DistilGPT-2) learned from a larger teacher model (DialoGPT-medium) to exhibit similar language generation capabilities while being much more efficient.

Teacher Model: Microsoft's DialoGPT-medium (larger, pre-trained conversational model)
Student Model: DistilGPT-2 (smaller, faster model, not pre-trained)

